<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>oneDNN Wrapper: Primitives</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">oneDNN Wrapper
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md_PRIMITIVES.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Primitives </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This page provides examples of how to use the implemented primitives and the general procedure to implement a new one.</p>
<h1><a class="anchor" id="autotoc_md8"></a>
General usage</h1>
<p>The primitive wrapper itself is quite simple, it is a class that exposes as public members the dnnl::memory objects associated with a primitive. For example, in a <a class="el" href="classDense.html" title="Dense allows to create a fully connected layer forward primitive.">Dense</a> layer we have:</p><ul>
<li>DNNL_ARG_SRC which is mapped to the public member <a class="el" href="classDense.html#a4c322957beac01d0becc79f81b12d3d7" title="Source memory handler.">Dense::arg_src</a></li>
<li>DNNL_ARG_DST which is mapped to the public member <a class="el" href="classDense.html#ad9af6e79858fa2910770bb1885081cb7" title="Destination memory handler.">Dense::arg_dst</a></li>
<li>DNNL_ARG_WEIGHTS which is mapped to the public member <a class="el" href="classDense.html#a27eb4fc10c99ede3277301e1d1d346fa" title="Weights memory handler.">Dense::arg_weights</a></li>
<li>DNNL_ARG_BIAS which is mapped to the public member <a class="el" href="classDense.html#af13071600533e6b567ec6438aa1e1392" title="Bias memory handler.">Dense::arg_bias</a></li>
</ul>
<p>Hence, this means that once we instantiate a <a class="el" href="classDense.html" title="Dense allows to create a fully connected layer forward primitive.">Dense</a> class we can simply call .arg_X to access any memory handler we need.</p>
<p>To instantiate a primitive it is sufficient to instantiate a class of the proper type, for example: </p><pre class="fragment">Dense fc1(fc1_output_size, input_memory, net_fwd, net_fwd_args, eng);
</pre><h1><a class="anchor" id="autotoc_md9"></a>
General implementation</h1>
<p>To implement a primitive coming from the oneDNN toolkit the first step is to read the documentation in order to understand what members we will have and what inputs we need to instantiate the class. Our specific design choice takes uses 5 header files to instantiate the primitives (or a combination of primitives for example in the case of a Loss Function):</p><ul>
<li><a class="el" href="layers__fwd_8h_source.html">layers_fwd.h</a> which contains the wrappers for the forward operations: <a class="el" href="classDense.html" title="Dense allows to create a fully connected layer forward primitive.">Dense</a>, Convolution...</li>
<li><a class="el" href="primitive__wrappers_8h_source.html">primitive_wrappers.h</a> which contains simple primitives such as <a class="el" href="classReorder.html" title="Reorder Primitive.">Reorder</a>, <a class="el" href="classEltwise.html" title="Primitive which provides element-wise operations.">Eltwise</a>...</li>
<li><a class="el" href="losses_8h_source.html">losses.h</a> which contains the loss functions as well as their gradients</li>
<li><a class="el" href="layers__bwd__data_8h_source.html">layers_bwd_data.h</a> which contains the wrapper for the backward data operations corresponding to the primitives declared in <a class="el" href="layers__fwd_8h_source.html">layers_fwd.h</a> and primitive_wrappers.cpp</li>
<li><a class="el" href="layers__bwd__weights_8h_source.html">layers_bwd_weights.h</a> which contains the wrapper for the backward weights operations corresponding to the primitives declared in <a class="el" href="layers__fwd_8h_source.html">layers_fwd.h</a> and primitive_wrappers.cpp</li>
</ul>
<p>Given the previous description, it is clear that depending on the primitive we are implementing we might need to implement the corresponding backward operation. We will now be looking at the example of the <a class="el" href="classDense.html" title="Dense allows to create a fully connected layer forward primitive.">Dense</a> layer.</p>
<h2><a class="anchor" id="autotoc_md10"></a>
EXAMPLE: Implementing a Dense Layer</h2>
<p>The <a class="el" href="classDense.html" title="Dense allows to create a fully connected layer forward primitive.">Dense</a> layer is the most complete example that allows us to explain the inner working of the oneDNN wrapper. First, we will implement the forward operation by declaring the prototype in the <a class="el" href="layers__fwd_8h_source.html">layers_fwd.h</a> file as follows </p><pre class="fragment">class Dense{
    public:
        dnnl::memory arg_src;
        dnnl::memory arg_dst; 
        dnnl::memory arg_bias; 
        dnnl::memory arg_weights; 
        Dense(int fc_output_size,
            dnnl::memory input,
            std::vector&lt;dnnl::primitive&gt; &amp;net,
            std::vector&lt;std::unordered_map&lt;int, dnnl::memory&gt;&gt; &amp;net_args,
            dnnl::engine eng);
    private:

};
</pre><p> The dnnl::memory variable instantiates an object that can be then associated to a memory location, the constructor <a class="el" href="classDense.html#ac84196f789601c3d5aea1a0022c4c29f" title="Construct a new Dense object.">Dense::Dense</a> takes as input all the information that we need to create the primitive. The actual forward operation is declared inside layers_fwd.cpp in a way that's very similar to the examples in oneDNN.</p>
<p>The first thing we do is obtain the dimensions from the input parameters </p><pre class="fragment">dnnl::memory::dims bias_dims_fc = {fc_output_size};
dnnl::memory::desc src_md_fc = input.get_desc(); 
dnnl::memory::dims src_dims_fc = src_md_fc.dims();
</pre><p> Here we have:</p><ul>
<li>bias_dims_fc which is simply a scalar that contains the output dimension</li>
<li>src_md_fc which is the memory::desc associated with the input</li>
<li>src_dims_fc which is the memory::desc::dims associated with the input. This vector contains the input dimension and can be indexed to obtain what we need.</li>
</ul>
<p>The weights dimensions require a bit more work, in fact the depend on the dimensions of our input, which can be 2D in the case of a simple network with only <a class="el" href="classDense.html" title="Dense allows to create a fully connected layer forward primitive.">Dense</a> layers, or 3D if coming from a convolutional layer. To address this use the following logic which exploits the src_dims_fc vector: </p><pre class="fragment">bool from_conv = (src_dims_fc.size() &gt; 3); 
if ( from_conv ){
    weights_dims_fc = {fc_output_size, src_dims_fc[1], src_dims_fc[2], src_dims_fc[3]};
}
else {
    weights_dims_fc = {fc_output_size, src_dims_fc[1]};
}
</pre><p> Now that we have all the input dimensions we can create all the necessary dnnl::memory::desc. The dnnl::memory::desc object can then be passed to the dnnl::memory constructor to allocate the memory inside the chosen engine: </p><pre class="fragment">auto bias_md_fc = dnnl::memory::desc(bias_dims_fc, dt::f32, tag::a);
auto dst_md_fc = dnnl::memory::desc(dst_dims_fc, dt::f32, tag::nc);
dnnl::memory::desc weights_md_fc;
if ( from_conv ){
    weights_md_fc = dnnl::memory::desc(weights_dims_fc, dt::f32, tag::oihw)
}
else {
    weights_md_fc = dnnl::memory::desc(weights_dims_fc, dt::f32, tag::oi);
}
auto src_mem_fc = dnnl::memory(src_md_fc, eng);
auto bias_mem_fc = dnnl::memory(bias_md_fc, eng);
auto weights_mem_fc = dnnl::memory(weights_md_fc, eng);
auto dst_mem_fc = dnnl::memory(dst_md_fc, eng);
</pre><p> The weights and biases are initialized using an RNG inside the local memory and are then written to the engine </p><pre class="fragment">std::vector&lt;float&gt; fc_weights(product(weights_dims_fc));
std::vector&lt;float&gt; fc_bias(product(bias_dims_fc));

for (int i = 0; i&lt;fc_weights.size(); i++){
    fc_weights[i] = norm_dist(generator);
}

for (int i = 0; i&lt;fc_bias.size(); i++){
    fc_bias[i] = norm_dist(generator);
    //std::cout &lt;&lt; fc_bias[i] &lt;&lt; " ";
}

write_to_dnnl_memory(fc_bias.data(), bias_mem_fc);
write_to_dnnl_memory(fc_weights.data(), weights_mem_fc);
</pre><p> The only interesting thing about this piece of code is the use of the product() function to obtain the "flattened" dimension and the write_to_dnnl_memory utility, which allows writing the float numbers in memory.</p>
<p>The primitive is created in a manner analogous to the memory, first, we create the dnnl::primitive::desc, then we instantiate the dnnl::primitive object </p><pre class="fragment">auto fc_desc = dnnl::inner_product_forward::desc(dnnl::prop_kind::forward_training, src_md_fc,
                                            weights_md_fc, bias_md_fc, dst_md_fc);
auto fc_pd = dnnl::inner_product_forward::primitive_desc(fc_desc, eng);
</pre><p>We then check all the types to see if they are compatible with the instantiated primitive, and if not create a <a class="el" href="classReorder.html" title="Reorder Primitive.">Reorder</a>: </p><pre class="fragment">src_mem_fc = checkType(fc_pd.src_desc(), input, net, net_args, eng);
weights_mem_fc = checkType(fc_pd.weights_desc(), weights_mem_fc, net, net_args, eng);
bias_mem_fc = checkType(fc_pd.bias_desc(), bias_mem_fc, net, net_args, eng);
</pre><p> The checkType has a different role in each case:</p><ul>
<li>In src_mem_fc we check if the input we are providing to the fully connected layer is compatible with the input expected by the FC;</li>
<li>In weights_mem_fc and bias_mem_fc we are simply checking if we instantiated the memory correctly, this helps a lot when debugging issues since it allows us to check if we correctly implemented our primitive, ideally, we should see no reorders for weights and biases;</li>
</ul>
<p>Finally, we expose the dnnl::memory objects by associating the pointers and append the primitive to the pipeline </p><pre class="fragment">arg_src = src_mem_fc;
arg_dst = dst_mem_fc;
arg_weights = weights_mem_fc;
arg_bias = bias_mem_fc;

net.push_back(dnnl::inner_product_forward(fc_pd));
net_args.push_back({{DNNL_ARG_SRC, src_mem_fc},
                    {DNNL_ARG_WEIGHTS, weights_mem_fc},
                    {DNNL_ARG_BIAS, bias_mem_fc},
                    {DNNL_ARG_DST, dst_mem_fc}});
</pre> </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1 </li>
  </ul>
</div>
</body>
</html>
