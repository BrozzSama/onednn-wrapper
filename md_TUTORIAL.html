<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>oneDNN Wrapper: Fully connected layer tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">oneDNN Wrapper
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md_TUTORIAL.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Fully connected layer tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial will cover all the aspects of training a simple neural network using oneDNN. It is divided into three parts and follows the structure of the example onednn_training_skin.cpp.</p>
<ul>
<li>Pipeline creation: which explains how to create the forward and backward streams, as well as how to update the weights</li>
<li>Output generation: this covers how to retrieve data from a oneAPI engine</li>
</ul>
<h1><a class="anchor" id="autotoc_md12"></a>
Forward pass</h1>
<p>The forward pass is built as follows: </p><pre class="fragment">    int fc1_output_size = 5;
    Dense fc1(fc1_output_size, input_memory, net_fwd, net_fwd_args, eng);
    Eltwise relu1(dnnl::algorithm::eltwise_relu, 0.f, 0.f, fc1.arg_dst,
                        net_fwd, net_fwd_args, eng);
    int fc2_output_size = 1;
    Dense fc2(fc2_output_size, relu1.arg_dst, net_fwd, net_fwd_args, eng);    
    Eltwise sigmoid1 (dnnl::algorithm::eltwise_logistic, 0.f, 0.f, fc2.arg_dst,
                        net_fwd, net_fwd_args, eng);
    binaryCrossEntropyLoss loss(sigmoid1.arg_dst, labels_memory, net_fwd, net_fwd_args, eng);
</pre><p> The pipeline is quite simple, first, we have a <a class="el" href="classDense.html" title="Dense allows to create a fully connected layer forward primitive.">Dense</a> layer that takes as input the input_memory onto which we wrote the data using the <a class="el" href="classDataLoader.html" title="DataLoader allows to create a dataloader object to implement minibatch stochastic gradient descent.">DataLoader</a> class. This first layer is activated by a relu function which is an <a class="el" href="classEltwise.html" title="Primitive which provides element-wise operations.">Eltwise</a> primitive. The output of the relu1 <a class="el" href="classEltwise.html" title="Primitive which provides element-wise operations.">Eltwise</a> is then passed to the second fully connected layers by using the Eltwise::arg_dst pubic member, which exposes the memory location of the output; again we use <a class="el" href="classEltwise.html" title="Primitive which provides element-wise operations.">Eltwise</a> to provide the activation function, in this case, we chose sigmoid. Finally, we have the <a class="el" href="classbinaryCrossEntropyLoss.html" title="Binary Cross Entropy Loss class.">binaryCrossEntropyLoss</a> which computed the loss starting from the probabilities of the sigmoid and the labels_memory.</p>
<h2><a class="anchor" id="autotoc_md13"></a>
Forward inference</h2>
<p>To have inference on validation we use the exact same scheme as before with different variable names.</p>
<h1><a class="anchor" id="autotoc_md14"></a>
Backward data pass</h1>
<p>The backward data pass is a bit more complex to understand due to the fact that it forces us to see the pipeline the other way around. </p><pre class="fragment">    binaryCrossEntropyLoss_back loss_back(sigmoid1.arg_dst, labels_memory, net_bwd_data, net_bwd_data_args, eng);
    Eltwise_back sigmoid1_back_data(dnnl::algorithm::eltwise_logistic, 0.f, 0.f, sigmoid1, 
                                     loss_back.arg_dst, net_bwd_data, net_bwd_data_args, eng);
    Dense_back_data fc2_back_data(sigmoid1_back_data.arg_diff_src, fc2, net_bwd_data, net_bwd_data_args, eng);
    Eltwise_back relu1_back_data(dnnl::algorithm::eltwise_relu, 0.f, 0.f, relu1, 
                                     fc2_back_data.arg_diff_src, net_bwd_data, net_bwd_data_args, eng);
    Dense_back_data fc1_back_data(relu1_back_data.arg_diff_src, fc1, net_bwd_data, net_bwd_data_args, eng);
</pre><p> First, we compute the gradient of the cross-entropy loss by providing the labels and the output of the sigmoid. This computation is passed as Eltwise_back::arg_diff_dst, since the <a class="el" href="classEltwise__back.html" title="Eltwise backward primitive.">Eltwise_back</a> operation will compute the Eltwise_back::arg_diff_src from it. The same operation is done with <a class="el" href="classDense__back__data.html" title="Dense layer backward data primitive.">Dense_back_data</a> in the case of fc2, and so on.</p>
<h1><a class="anchor" id="autotoc_md15"></a>
Backward weights pass</h1>
<p>The backward weights pass can be put in any order we want since they are independent computations, in this specific scenario we simply preserve the order from the backward data pass. </p><pre class="fragment">Dense_back_weights fc2_back_weights(sigmoid1_back_data.arg_diff_src, fc2, net_bwd_weights, net_bwd_weights_args, eng);
Dense_back_weights fc1_back_weights(relu1_back_data.arg_diff_src, fc1, net_bwd_weights, net_bwd_weights_args, eng);
</pre> <h1><a class="anchor" id="autotoc_md16"></a>
Weights update</h1>
<p>To update the gradient through SGD we can use the updateWeights_SGD class, which simply subtracts the gradient multiplied by the learning rate to each weight and bias. </p><pre class="fragment">    updateWeights_SGD(fc1.arg_weights, 
               fc1_back_weights.arg_diff_weights,
               learning_rate, net_sgd, net_sgd_args, eng);
    Reorder(fc1.arg_weights, fc1_inf.arg_weights,
               net_bwd_weights, net_bwd_weights_args, eng);
    updateWeights_SGD(fc2.arg_weights, 
               fc2_back_weights.arg_diff_weights,
               learning_rate, net_sgd, net_sgd_args, eng);
    Reorder(fc2.arg_weights, fc2_inf.arg_weights,
               net_bwd_weights, net_bwd_weights_args, eng);
    updateWeights_SGD(fc1.arg_bias, 
               fc1_back_weights.arg_diff_bias,
               learning_rate, net_sgd, net_sgd_args, eng);
    Reorder(fc1.arg_bias, fc1_inf.arg_bias,
               net_bwd_weights, net_bwd_weights_args, eng);
    updateWeights_SGD(fc2.arg_bias, 
               fc2_back_weights.arg_diff_bias,
               learning_rate, net_sgd, net_sgd_args, eng);
    Reorder(fc2.arg_bias, fc2_inf.arg_bias,
               net_bwd_weights, net_bwd_weights_args, eng);
</pre><p> The syntax is pretty intuitive, the only tricky part to understand here is the use of the <a class="el" href="classReorder.html" title="Reorder Primitive.">Reorder</a> primitive. In this specific scenario, we have a <a class="el" href="classReorder.html" title="Reorder Primitive.">Reorder</a> for each weight and bias vector because we want to propagate the changes made in training to the validation pipeline.</p>
<h1><a class="anchor" id="autotoc_md17"></a>
Training Loop</h1>
<p>Finally, once the pipelines are ready, it is time to train our network. We simply have a loop that iterates the training n_iter times. </p><pre class="fragment">while (n_iter &lt; max_iter)
    {

            for (size_t i = 0; i &lt; net_fwd.size(); ++i)
                    net_fwd.at(i).execute(s, net_fwd_args.at(i));

            for (size_t i = 0; i &lt; net_bwd_data.size(); ++i)
                    net_bwd_data.at(i).execute(s, net_bwd_data_args.at(i));
            for (size_t i = 0; i &lt; net_bwd_weights.size(); ++i)
                    net_bwd_weights.at(i).execute(s, net_bwd_weights_args.at(i));
            for (size_t i = 0; i &lt; net_sgd.size(); ++i)
                    net_sgd.at(i).execute(s, net_sgd_args.at(i));

            if (n_iter % step == 0){  
                    for (size_t i = 0; i &lt; net_fwd_inf.size(); ++i)
                            net_fwd_inf.at(i).execute(s, net_fwd_inf_args.at(i));

            }

            skin_data.write_to_memory(input_memory, labels_memory);

            n_iter++;
    }
</pre><p> the dnnl::primitive::execute method of each primitive executed the primitive for each pipeline. Once an iteration is fully complete we use <a class="el" href="classDataLoader.html#a333e4be57bbaaa78f003553e4a4f5632" title="Method that writes the curr batch to memory and moves the index forward.">DataLoader::write_to_memory</a> to change the batch.</p>
<h1><a class="anchor" id="autotoc_md18"></a>
Changing parameters</h1>
<p>Through the included nholman::json library it is possible to parametrize your network and avoid recompiling the code every time there is a minor change. Currently the onednn_training_skin.cpp example supports:</p>
<ul>
<li>iterations: which controls the number of iterations</li>
<li>dataset_path: which provides the path to the feature file</li>
<li>labels_path: which provides the path to the labels</li>
<li>dataset_path_val: which provides the path to the feature file for validation</li>
<li>labels_path_val: which provides the path to the labels file for validation</li>
<li>loss_filename: which provides the path for the loss vector that will be produced in the output</li>
<li>loss_inf_filename: which provides the path for the validation loss vector that will be produced in the output</li>
<li>val_predicted_filename: which provides the predictions on the validation set</li>
<li>minibatch_size: which provides the batch size</li>
<li>step: which provides after how many iterations the data should be printed</li>
<li>learning_rate: which provides the learning rate for SGD</li>
</ul>
<h1><a class="anchor" id="autotoc_md19"></a>
Debugging</h1>
<p>Unfortunately debugging is not available on GPU. To print the output of a vector inside a oneAPI engine we can use the read_from_dnnl_memory() function from <a class="el" href="intel__utils_8h_source.html">intel_utils.h</a> that moves data from oneAPI engine inside the RAM. To print a vector one can use the print_vector utility. Note that once read_from_dnnl_memory() is called, one must also use the wait() method on the engine, in order to wait for all the queued operations to finish.</p>
<h1><a class="anchor" id="autotoc_md20"></a>
Performance measurements</h1>
<p>By setting the DNNL_VERBOSE=1 flag in the run script as follows: </p><pre class="fragment">    DNNL_VERBOSE=1 ./dpcpp/onednn-training-$1-cpp gpu $2
</pre><p> it is possible to enable verbose output. What this means is that oneAPI will provide additional information about all of the operations that are being run on the engine. This is particularly useful since it allows to check if all the memory types are proper and additionally it allows to compare performance since it provides the execution time. It must be noted that the output is essentially a non-standard CSV format, therefore after some cleaning up it can be analyzed using pandas. To do so it is sufficient to strip all the additional output by means of the grep command cat run_cpu_xxx | grep dnnl_verbose &gt; my_clean_output.log Once this is done to have a file that is readable by pandas it is sufficient to remove the first few lines that only tell us what engines are available: dnnl_verbose,info,oneDNN v2.2.0 (commit 7489a2ea4c14c94f5bbe7d5fc774f722444cdd3f) dnnl_verbose,info,cpu,runtime:DPC++ dnnl_verbose,info,cpu,isa:Intel AVX-512 with Intel DL Boost dnnl_verbose,info,gpu,runtime:DPC++ dnnl_verbose,info,cpu,engine,0,backend:OpenCL,name:Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz,driver_version:2021.11.3 dnnl_verbose,info,gpu,engine,0,backend:Level Zero,name:Intel(R) Iris(R) Xe MAX Graphics [0x4905],driver_version:1.0.19310 dnnl_verbose,info,gpu,engine,1,backend:Level Zero,name:Intel(R) Iris(R) Xe MAX Graphics [0x4905],driver_version:1.0.19310 dnnl_verbose,info,gpu,engine,2,backend:Level Zero,name:Intel(R) Iris(R) Xe MAX Graphics [0x4905],driver_version:1.0.19310 And then edit the header by modifying: </p><pre class="fragment">    dnnl_verbose,info,prim_template:operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time
</pre><p> into: dnnl_verbose,operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time</p>
<p>Now using pandas we can import the data frame and do analysis. Some examples are available in the analysis.ipynb Jupyter Notebook. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1 </li>
  </ul>
</div>
</body>
</html>
